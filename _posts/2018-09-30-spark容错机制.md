---
layout:     post
title:      SPARK的数据容错机制
subtitle:   SPARK的数据容错机制
date:       2018-09-30
author:     danfeng
header-img: img/post-bg-ios10.jpg
catalog: 	 true
tags:
    - SPARK
    - BIGDATA
---       



# spark的数据容错机制：
一般而言，分布式数据集的容错性具备两种方式：**数据检查点和记录数据的更新**
- checkpoint机制——数据检查点
- 记录更新机制（在Saprk中对应Lineage机制）

###  &emsp;下面对这两种机制分别进行介绍：
## checkpoint机制
-  checkpoint的意思是建立检查点，类似于快照，传统的Spark任务计算过程中，DAG特别长，集群需要将整个DAG计算完成得到结果，但是如果在这个漫长的计算过程中出现数据丢失，Spark又会根据依赖关系重新从头开始计算一遍结果，这样很浪费性能，当然我们可以考虑将中间计算结果cache或者persist到内存或者磁盘中，但是这样不能保证数据完全不丢失，存储的内存出现了问题或者磁盘坏掉，也会导致Spark再次根据RDD重新再次计算一遍，所以就出现了checkpoint机制，其中checkpoint的作用就是将DAG中比较重要的中间计算结果存储到一个高可用的地方，通常这个地方是HDFS里面。
   <br>&emsp;&emsp;另外需要注意cache和checkpoint机制之间的区别：
    1. 检查点是新建一个job来完成的，是执行的完一个job之后，新建一个来完成的；而cache，是job执行过程中进行。 
    2. 检查点对RDD的checkpoint是将数据的血统截断，只保存了想要保存的RDD在HDFS中，而cache的是计算血统的数据在内存中。
    3. 缓存的清除方式也不一样，checkpoint到HDFS中的RDD需要手动清除，如果不手动清除，会一直存在，可以被下一个驱动程序所使用；而cache到内存和persist到磁盘的partition， 由 blockManager 管理。一旦 driver program 执行结束，也就是 executor 所在进程 CoarseGrainedExecutorBackend stop，blockManager 也会 stop，被 cache 到磁盘上的 RDD 也会被清空（整个 blockManager 使用的 local 文件夹被删除）。
 ###  checkpoint机制不足
1.   操作成本高，需要通过数据中心的网络连接在机器之间复制庞大的数据集，而网络带宽往往比内存带宽低得多，同时还需要消耗更多的存储资源。因此Spark侧重于记录更新的方式。


## Lineage机制
 &emsp;&emsp;RDD只支持粗粒度转换，即只记录单个块上执行的单个操作，然后将创建RDD的一系列变换序列（每个RDD都包含了他是如何由其他RDD变换过来的以及如何重建某一块数据的信息。因此RDD的容错机制又称“血统(Lineage)”容错）记录下来，以便恢复丢失的分区。 
Lineage本质上很类似于数据库中的重做日志（Redo Log），只不过这个重做日志粒度很大，是对全局数据做同样的重做进而恢复数据。

  &emsp;&emsp;相比其他系统的细颗粒度的内存数据更新级别的备份或者LOG机制，RDD的Lineage记录的是粗颗粒度的特定数据Transformation操作（如filter、map、join等）行为。当这个RDD的部分分区数据丢失时，它可以通过Lineage获取足够的信息来重新运算和恢复丢失的数据分区。因为这种粗颗粒的数据模型，限制了Spark的运用场合，所以Spark并不适用于所有高性能要求的场景，但同时相比细颗粒度的数据模型，也带来了性能的提升。
  
  &emsp;&emsp;Rdd在Lineage依赖方面划分成两种依赖：**窄依赖**（Narrow Dependencies)与**宽依赖**，根据父RDD分区是对应1个还是多个子RDD分区来区分窄依赖（父分区对应一个子分区）和宽依赖（父分区对应多个子分 
区）。
   
```
窄依赖：指父RDD的每一个分区最多被一个子RDD的分区所用，表现为一个父RDD的分区对应于一个子RDD的分区，和两个父RDD的分区对应于一个子RDD 的分区。图中，map/filter和union属于第一类，对输入进行协同划分（co-partitioned）的join属于第二类(这个主要表现在spark还支持像**groupbyke**和**reducebykey**这样的宽依赖，这样计算子RDD中一个partition的一条记录依赖父RDD中许多partition中的记录，具有相同key的所有tuple最终会放到在同一个partition中，然后被同一个task处理，为了满足这种操作，spark必须进行shuffle,通过在集群的节点间传输数据最终生成一个新的stage和新的partition集合。（同一个stage中所有的RDD的partition数据应该不一致）)。

宽依赖：
指子RDD的分区依赖于父RDD的所有分区，这是因为shuffle类操作，如图中的groupByKey和未经协同划分的join。

 下面以单词统计为例进行分析：
 val tokenized = sc.textFile(args(0)).flatMap(_.split(' '))
 val wordCounts = tokenized.map((_,1)).reduceByKey(_ + _)
 val filtered = wordCounts.filter(_._2 >=1000)
 val charCounts = filtered.flatMap(_._1.toCharArray).map((_,1)).
 reduceByKey(_ + _)
 charCounts.collect()
 上面代码将会分成3个stage进行执行，reduceByKey操作会产生一个stage边界，因为计算它的输出需要数据按key进行重新分区
```
![image](https://zdfccdanfeng.github.io/img/na.png)
![image](https://zdfccdanfeng.github.io/img/ii.png)
![image](https://zdfccdanfeng.github.io/img/ii2.png)

-    

 - **第一**，窄依赖可以在某个计算节点上直接通过计算父RDD的某块数据计算得到子RDD对应的某块数据；宽依赖则要等到父RDD所有数据都计算完成之后，并且父RDD的计算结果进行hash并传到对应节点上之后才能计算子RDD。 
- **第二**，数据丢失时，对于窄依赖只需要重新计算丢失的那一块数据来恢复；对于宽依赖则要将祖先RDD中的所有数据块全部重新计算来恢复。所以在长“血统”链特别是有宽依赖的时候，需要在适当的时机设置数据检查点。也是这两个特性要求对于不同依赖关系要采取不同的任务调度机制和容错恢复机制。
###  容错原理
 在容错机制中，如果一个节点死机了，而且运算窄依赖，则只要把丢失的父RDD分区重算即可，不依赖于其他节点。而宽依赖需要父RDD的所有分区都存在，重算就很昂贵了。可以这样理解开销的经济与否：在窄依赖中，在子RDD的分区丢失、重算父RDD分区时，父RDD相应分区的所有数据都是子RDD分区的数据，并不存在冗余计算。在宽依赖情况下，丢失一个子RDD分区重算的每个父RDD的每个分区的所有数据并不是都给丢失的子RDD分区用的，会有一部分数据相当于对应的是未丢失的子RDD分区中需要的数据，这样就会产生冗余计算开销，这也是宽依赖开销更大的原因。因此如果使用Checkpoint算子来做检查点，不仅要考虑Lineage是否足够长，也要考虑是否有宽依赖，对宽依赖加Checkpoint是最物有所值的。




  



 

    
