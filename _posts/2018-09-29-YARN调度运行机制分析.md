---
layout:     post
title:      YARN运行机制分析
subtitle:   YARN运行机制分析
date:       2018-09-29
author:     danfeng
header-img: img/post-bg-ios10.jpg
catalog: 	 true
tags:
    - HADOOP
    - BIGDATA
---       



###  YARN调度运行机制
&emsp;&emsp;在传统的MapReduce中, Jobtracker同时负责作业调度(将任务调度给对应的tasktracker)和任务进度管理(监控任务, 重启失败的或者速度比较慢的任务等). YARN中将Jobtracker的责任划分给两个独立的守护进程: 资源管理器(resource manager)负责管理集群的所有资源, 应用管理器(application master)负责管理集群上任务的生命周期. 具体的做法是应用管理器向资源管理器提出资源需求, 以container为单位, 然后在这些container中运行该应用相关的进程.
 container由运行在集群节点上的节点管理器监控, 确保应用不会用超资源. 每个应用的实例, 亦即一个MapReduce作业都有一个自己的应用管理器. 
&emsp;&emsp;YARN是运算资源的调度系统。运算资源包括运行程序的jar包，配置文件，CPU，内存，IO等。使用了linux的资源隔离机制cgroup实现了CPU和内存的隔离。它的运行容器叫做container。每个container中包含了一定的CPU+内存。docker，openstack等虚拟化框架都使用了cgroup。首先，客户端启动后获取一个YARNRunner,它本质上是一个动态代理对象。它负责将任务提交到YARN集群中。
 
 &emsp;&emsp;&emsp;先上一个整体的流程图：
#   ![image](https://zdfccdanfeng.github.io/img/RMMMOV1.png) 
# ![image](https://zdfccdanfeng.github.io/img/mr.png)
  
```
（0）Mr程序提交到客户端所在的节点

（1）yarnrunner向Resourcemanager申请一个application。

（2）rm将该应用程序的资源路径返回给yarnrunner
 
（3）该程序将运行所需资源提交到HDFS上

（4）程序资源提交完毕后，申请运行mrAppMaster

（5）RM将用户的请求初始化成一个task
 
（6）其中一个NodeManager领取到task任务。
 
（7）该NodeManager创建容器Container，并产生MRAppmaster

（8）Container从HDFS上拷贝资源到本地

（9）MRAppmaster向RM
申请运行maptask容器
 
（10）RM将运行maptask任务分配给另外两个NodeManager，另两个NodeManager分别领取任务并创建容器。

（11）MR向两个接收到任务的NodeManager发送程序启动脚本，这两个NodeManager分别启动maptask，maptask对数据分区排序。

（12）MRAppmaster向RM申请2个容器，运行reduce task。
 
（13）reduce task向maptask获取相应分区的数据。

（14）程序运行完毕后，MR会向RM注销自己。
```

### YARN的重要概念：
&emsp;&emsp;1）yarn并不清楚用户提交的程序的运行机制

&emsp;&emsp;2）yarn只提供运算资源的调度（用户程序向yarn申请资源，yarn就负责分配资源）

&emsp;&emsp;3）yarn中的主管角色叫ResourceManager

&emsp;&emsp;4）yarn中具体提供运算资源的角色叫NodeManager

&emsp;&emsp;5）这样一来，yarn其实就与运行的用户程序完全解耦，就意味着yarn上可以运行各种类型的分布式运算程序（mapreduce只是其中的一种），比如mapreduce、storm程序，spark程序……

&emsp;&emsp;6）所以，spark、storm等运算框架都可以整合在yarn上运行，只要他们各自的框架中有符合yarn规范的资源请求机制即可

&emsp;&emsp;7）Yarn就成为一个通用的资源调度平台，从此，企业中以前存在的各种运算集群都可以整合在一个物理集群上，提高资源利用率，方便数据共享Yarn工作机制

&emsp;下面分析下提交过程：YARNRunner会去找ResourceManager，申请提交一个Application，ResourceManager会返回一个Application资源提交路径hdfs://xxx/.staging以及job_id。YARNRunner将资源上传到ResourceManager指定的HDFS目录中。包含job.split,job.xml.job.jar。然后通知ResourceManager，任务提交完成了。

1. -由于ResourceManager会接收很多程序，而计算资源是有限的。因此不能保证每个任务一提交就能运行，所以需要有一个调度机制。（调度策略包括FIFO，Fair，Capacity等。）当Job提交成功后，ResourceManager把Job封装成一个Task。
2. NodeManager与ResourceManager通信时，会领取这个Task。领取到Task后，NodeManager根据Task的描述，生成任务运行的容器container。并从HDFS上把任务需要的文件下载下来，放到container的工作目录中。启动MRAppmaster。
3. MRAppmaster请求ResourceManager分配若干个Container来启动MapTask。ResourceManager同样将Task放入队列中。NodeManager与ResourceManager通信时，会领取这个Task。NodeManager会分配一个容器。
4. 然后MRAppmaster会发送启动程序的脚本给NodeManager。MapTask就运行起来了。MRAppmaster负责监管MapTask，如果MapTask失败了，MRAppmaster会申请再运行一个MapTask。等到MapTask运行完毕之后，输出结果保存在container的工作目录下面。
5. MRAppmaster再申请容器运行ReduceTask。ReduceTask运行起来以后，会去下载MapTask的输出结果，每个ReduceTask获取自己负责的那一部分。ReduceTask执行完毕后，MRAppmaster会向ResourceManager注销自己，YARN会回收所有的计算资源。
##  总结
YARN现状
YARN只负责程序运行所需要资源的分配回收等调度任务，和MapReduce程序没有什么耦合。所以许许多多的其他的程序也可以在YARN上运行，比如说Spark，Storm等。
Hadoop中的JobTracker。Hadoop1中没有YARN，它使用JobTracker和TaskTracker。客户端提交任务给JobTracker，JobTracker负责启动MapTask和ReduceTask。
JobTracker知道我们的程序是怎么运行的，即JobTracker和MR程序是紧紧耦合在一起的。JobTracker只有一个节点，要负责：资源调度
应用的运算流程管理监控所以负担很重，如果JobTracker挂了，所有的程序都不能运行了。对比看来，明显是YARN的架构更好。







  
  

###  YARN调度运行机制
